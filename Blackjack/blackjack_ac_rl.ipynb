{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Critic Reinforcement Learning Method\n",
    "https://keras.io/examples/rl/actor_critic_cartpole/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from blackjack_env import BlackjackEnv\n",
    "\n",
    "# Custom blackjack environment\n",
    "env = BlackjackEnv()\n",
    "\n",
    "# Configuration parameters for the whole setup\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "num_inputs = 2 # Player hand value, Dealer's visible card\n",
    "num_actions = 2 # Hit or stay\n",
    "num_hidden = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Actor-Critic Model\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "# Implement Actor Critic network\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "huber_loss = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Running Reward: -1.00\n",
      "Episode 50: Running Reward: -0.04\n",
      "Episode 100: Running Reward: -0.19\n",
      "Episode 150: Running Reward: -0.25\n",
      "Episode 200: Running Reward: -0.26\n",
      "Episode 250: Running Reward: -0.26\n",
      "Episode 300: Running Reward: -0.30\n",
      "Episode 350: Running Reward: -0.31\n",
      "Episode 400: Running Reward: -0.33\n",
      "Episode 450: Running Reward: -0.29\n",
      "Episode 500: Running Reward: -0.30\n",
      "Episode 550: Running Reward: -0.29\n",
      "Episode 600: Running Reward: -0.28\n",
      "Episode 650: Running Reward: -0.29\n",
      "Episode 700: Running Reward: -0.29\n",
      "Episode 750: Running Reward: -0.29\n",
      "Episode 800: Running Reward: -0.29\n",
      "Episode 850: Running Reward: -0.30\n",
      "Episode 900: Running Reward: -0.30\n",
      "Episode 950: Running Reward: -0.30\n",
      "Episode 1000: Running Reward: -0.30\n",
      "Average Actor Loss: -0.3249\n",
      "Average Critic Loss: 0.2976\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "MAX_HANDS = 1000 # Stops after 1,000 episodes\n",
    "epsilon = 0.1 # Exploration factor\n",
    "\n",
    "# Track actor and critic losses\n",
    "actor_losses_history = []\n",
    "critic_losses_history = []\n",
    "\n",
    "for hand_count in range(MAX_HANDS):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    action_probs_history, critic_value_history, rewards_history = [], [], []\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        for _ in range(100):  # Play rounds\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "            action_probs, critic_value = model(state_tensor)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(num_actions) # Explore\n",
    "            else:\n",
    "                action = np.argmax(action_probs) # Exploit\n",
    "\n",
    "            # Choose an action based on probabilities\n",
    "            # action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # Apply the action in Blackjack\n",
    "            state, reward, done = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break # End episode\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in reversed(rewards_history):\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize returns\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-7)\n",
    "\n",
    "        # Compute Loss\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses, critic_losses = [], []\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up receiving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # Actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Track losses for averaging after training\n",
    "    actor_losses_history.append(np.mean(actor_losses))\n",
    "    critic_losses_history.append(np.mean(critic_losses))\n",
    "\n",
    "    # Track performance\n",
    "    running_reward += episode_reward\n",
    "    #running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward # Higher value - model reacts faster to changes / Lower value - smoother trend but slower to update\n",
    "    episode_count += 1\n",
    "\n",
    "    if episode_count % 50 == 0 or episode_count == 1:\n",
    "        print(f\"Episode {hand_count + 1}: Running Reward: {running_reward / episode_count:.2f}\")\n",
    "\n",
    "    if running_reward / episode_count > 5:  # Stop when the AI consistently wins\n",
    "        print(f\"Solved in {hand_count + 1} episodes!\")\n",
    "        break\n",
    "\n",
    "# Display average loss after episodes complete\n",
    "average_actor_loss = np.mean(actor_losses_history)\n",
    "average_critic_loss = np.mean(critic_losses_history)\n",
    "\n",
    "print(f\"Average Actor Loss: {average_actor_loss:.4f}\")\n",
    "print(f\"Average Critic Loss: {average_critic_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
